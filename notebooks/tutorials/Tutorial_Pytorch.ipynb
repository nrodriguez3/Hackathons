{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a PyTorch\n",
    "\n",
    "Contenidos:\n",
    "* Que es PyTorch\n",
    "* Comandos básicos\n",
    "* Ejemplos de la Primera Tarea\n",
    "* Ejemplos para la segunda Tarea\n",
    "\n",
    "\n",
    "## Que es PyTorch\n",
    "\n",
    "Objetivo:\n",
    "* Calculos con tensores con soporte para GPU\n",
    "* Sistema auto-diferenciable para DNN\n",
    "\n",
    "### Tensores\n",
    "* Trabajar con ndarray (numpy) es trabajar con tensores, son vectores de multiples dimensiones.\n",
    "* PyTorch permite que estos tensores puedan vivir tanto CPU con GPU, acelerando los calculos\n",
    "* Se proveen diferentes funciones, al igual que en numpy\n",
    "\n",
    "### Grafos dinamicos\n",
    "* Puedes modificar (dentro de lo posible) el comportamiento de la red sin partir de cero\n",
    "* Ya son varias las librerias que estan adoptando esta misma idea\n",
    " * PyTorch no fue el primero\n",
    " \n",
    " \n",
    "## Comandos Basicos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9669,  0.2019,  0.0537],\n",
      "        [ 0.6339,  0.9273,  0.6596],\n",
      "        [ 0.1822,  0.1147,  0.1867],\n",
      "        [ 0.7765,  0.8504,  0.7440],\n",
      "        [ 0.5203,  0.0558,  0.4122]])\n",
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x)\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  2,  3,  4])\n",
      "tensor([  3,   6,   9,  12])\n",
      "tensor([  1,   4,   9,  16])\n",
      "tensor([ 2,  4,  6,  8])\n",
      "tensor([ 2,  4,  6,  8])\n"
     ]
    }
   ],
   "source": [
    "y = torch.tensor([1,2,3,4])\n",
    "print(y)\n",
    "print(y*3)\n",
    "print(y*y)\n",
    "print(y.add_(y))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
    "print(x.size(), y.size(), z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# at beginning of the script\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# then whenever you get a new Tensor or Module\n",
    "# this won't copy if they are already on the desired device\n",
    "y = torch.ones_like(x, device=device)\n",
    "print(y.dtype) # torch.cuda.FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplos de la Primera Tarea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_resnet = models.resnet18(pretrained=False)\n",
    "print(model_resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Dropout(p=0.5)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "model_urls = {\n",
    "    'alexnet': 'https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth',\n",
    "}\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), 256 * 6 * 6)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def forward2(self, x):\n",
    "        for param in self.features:\n",
    "            x = param(x)\n",
    "        x = x.view(x.size(0), 256 * 6 * 6)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "def alexnet(pretrained=False, model_root=None, **kwargs):\n",
    "    model = AlexNet(**kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['alexnet'], model_root))\n",
    "    return model\n",
    "\n",
    "model_alexnet = alexnet(pretrained=True)\n",
    "print(model_alexnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uno puede interar sobre las capas,\n",
    "\n",
    "Esto con el fin de realizar operaciones sobre estas o verificar el tamaño de los pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 11, 11])\n",
      "torch.Size([192, 64, 5, 5])\n",
      "torch.Size([384, 192, 3, 3])\n",
      "torch.Size([256, 384, 3, 3])\n",
      "torch.Size([256, 256, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "for param in model_alexnet.features: #model_alexnet.parameters\n",
    "    if isinstance(param,nn.Conv2d):\n",
    "        print(param.weight.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7552, -0.1689, -0.9439, -0.5384,  1.1559,  0.8958,  0.5463,\n",
      "          0.2871, -0.1313, -0.7396, -1.0388,  0.3761,  0.8632,  0.6398,\n",
      "         -0.1557, -1.0014, -1.0093,  0.0139, -0.0205,  0.0312, -2.7950,\n",
      "         -1.5705, -2.2777, -0.7017, -0.9930, -1.3842, -0.3272,  0.5274,\n",
      "         -0.1193,  0.1863, -0.2608,  0.9001, -1.0014, -1.1853,  0.4133,\n",
      "         -0.6859, -0.1114,  0.4628,  0.9162,  0.1040,  1.5633, -1.1323,\n",
      "          0.4904, -0.0816,  0.4723, -0.3235,  2.2951,  0.9333,  0.5476,\n",
      "         -0.1700, -0.9409, -1.4243, -0.0917, -1.2227, -0.9893,  0.6052,\n",
      "         -1.7847, -0.8357,  0.3175,  0.4079,  0.9060, -0.8474, -1.3600,\n",
      "         -0.6324,  0.2319, -1.2032, -0.9547, -0.7137, -0.2251, -0.9471,\n",
      "          1.0247, -0.6131, -0.1351, -0.8512, -1.1499,  0.3398,  0.9506,\n",
      "          0.7957,  2.4142,  1.1020, -1.5938, -1.1928, -1.0387, -2.0637,\n",
      "          0.3256, -0.0194, -1.1271, -0.6633,  1.1098, -0.1492,  1.0599,\n",
      "         -1.1323, -0.4617,  0.5242,  1.6831, -2.0778,  1.7609, -0.1047,\n",
      "         -1.8564,  0.0199,  0.5258, -0.0719, -1.3181, -0.2438,  0.0888,\n",
      "         -1.7426, -0.3140,  1.5837, -0.7743,  0.9735, -0.2202,  2.3170,\n",
      "         -1.0533, -0.1358, -0.2465, -0.6431, -1.8146,  0.1448,  1.8233,\n",
      "          0.1651, -1.0025, -0.3474, -0.8193,  1.3130,  1.0830,  0.7557,\n",
      "         -0.5592, -0.3985,  0.2126, -0.1296, -0.1912, -0.5706, -0.6013,\n",
      "         -0.9768, -0.4623, -2.9671, -0.4551,  0.3573, -1.7873, -1.0840,\n",
      "         -3.4342, -2.1353, -2.3154,  0.2316, -1.6169, -0.4329, -1.4863,\n",
      "         -2.9832,  0.0010, -0.8183, -2.4628,  1.2865, -2.4152,  0.4376,\n",
      "         -0.9675,  0.4987, -1.3764, -0.7910, -0.8086,  0.0660, -0.7126,\n",
      "          1.3232,  1.3896,  0.2852, -0.3611, -0.2461, -0.5293, -2.0047,\n",
      "          0.7961, -0.7232, -0.9023,  0.7261, -1.2357, -0.1092,  0.9391,\n",
      "         -3.0977, -1.5966, -1.6215, -0.3509, -0.8810,  1.0028, -1.3969,\n",
      "          0.4025,  0.2125, -1.0809, -0.9282, -0.4065,  0.7259, -2.1559,\n",
      "          0.4798, -1.0663,  1.1314,  1.1073,  0.5613, -1.3240,  2.3820,\n",
      "          0.1607,  0.0082, -0.9067,  1.8682, -1.5168,  0.4929,  1.4767,\n",
      "          1.4288,  0.7216,  0.8053, -1.0157, -0.0972, -0.8284, -1.1952,\n",
      "         -2.1885, -0.8938, -0.6508, -0.3841, -0.2744, -0.1502, -0.4257,\n",
      "         -0.4293, -0.1293,  0.0983, -0.7157, -0.3942, -1.3974,  1.4047,\n",
      "          0.6266, -0.5477,  0.1123,  0.5728, -2.2013, -0.6798,  0.5824,\n",
      "          0.3097,  0.2131, -2.2212,  0.2953, -0.6582,  1.1369,  0.5941,\n",
      "          0.3756, -0.1559,  0.5091, -0.4616,  0.4332, -1.0783, -2.7326,\n",
      "         -0.0272, -1.9531, -0.4437, -0.4438, -0.8183, -0.0894, -0.3428,\n",
      "         -1.0557,  0.5045,  0.0234, -0.8732, -0.1797, -0.3878, -0.6339,\n",
      "         -0.3661, -0.0244, -0.1635, -1.4965,  0.3655, -0.4653,  0.8571,\n",
      "         -0.8843, -1.1566, -1.7972, -0.0156, -0.2117,  0.0097, -0.3886,\n",
      "         -1.0627, -1.1196, -1.8165, -2.8010, -0.7107, -0.8317, -0.8353,\n",
      "          0.1782,  0.3645,  1.7569, -0.3202,  1.1793, -0.9782, -0.3869,\n",
      "         -0.9159, -2.0166, -1.3165, -2.2532, -1.7214, -1.5694, -1.6738,\n",
      "         -1.1363,  1.9765, -1.9462, -0.0549, -1.5009, -2.4778,  0.3113,\n",
      "          2.3620,  0.0205,  0.1590,  1.2944, -0.2362,  0.5454,  0.5677,\n",
      "          0.2773, -0.6807,  1.2515,  1.5809,  1.0528,  1.1585,  2.0051,\n",
      "          1.6313,  1.5504,  2.9588,  3.6454,  0.7238, -0.6295, -1.5666,\n",
      "         -1.9749, -1.7666, -2.0450, -0.8938, -1.4012,  1.9227,  1.9393,\n",
      "         -0.3023,  0.6874, -0.6945,  0.0622, -0.4723, -0.9702, -1.0259,\n",
      "         -1.9462, -1.4050,  0.1002, -0.8719, -2.2377, -0.6746, -1.6516,\n",
      "         -1.7156, -1.5285, -1.2539, -1.9294, -2.3834, -2.2795, -0.9465,\n",
      "         -1.6515, -1.8804, -1.5682, -0.8300, -0.0478, -1.5815, -0.8050,\n",
      "         -0.0671, -0.2656,  0.7316, -1.5283,  1.0417, -0.6033,  0.7072,\n",
      "         -2.3236,  0.3833, -0.8487, -0.2696, -1.3128, -0.8735,  0.1570,\n",
      "         -1.8800, -0.6600, -1.3540, -1.3018, -0.7907, -1.5376, -0.6333,\n",
      "         -1.9030, -0.6005, -0.3627, -1.2882, -0.6191, -1.4905, -2.5510,\n",
      "         -0.1879, -2.1877, -0.7662, -0.0634, -1.4076,  1.8917,  0.4219,\n",
      "          0.0360,  0.3703,  0.2390,  0.2399,  0.1924,  0.1694,  0.0255,\n",
      "          3.1199,  0.0463,  1.3328,  1.1810, -1.2397, -2.8410, -0.8069,\n",
      "         -1.0235, -2.1014, -3.0537,  0.9347,  0.9975,  4.7823,  1.5087,\n",
      "          1.1118,  1.8190, -1.3354,  0.0551,  1.2593,  0.9583,  1.7380,\n",
      "          0.1678, -0.3213, -0.6583, -0.1732, -0.9861, -0.6386, -0.0526,\n",
      "         -0.0781,  0.4360, -0.5627, -1.2091,  2.0285,  1.5514, -0.7898,\n",
      "          0.9964,  0.4715, -1.4372, -1.5665,  0.1046, -0.1789,  1.2456,\n",
      "         -0.3145, -2.0356,  3.8388, -0.9953, -1.2109,  4.8983,  0.3548,\n",
      "          0.5745, -1.2080, -2.0586,  2.2976,  3.1739, -0.1234, -0.6029,\n",
      "          2.6906, -1.1706,  1.5209,  0.0282,  0.5257, -0.6637,  1.3036,\n",
      "          1.3756,  0.5795,  1.2941,  0.9895, -1.6332, -2.4373,  0.0822,\n",
      "          0.8179, -0.5032, -1.8408, -1.5806, -0.2714,  4.3713,  0.6248,\n",
      "         -2.7737,  0.2452,  1.0051, -0.2565, -0.0932,  1.5250,  1.0868,\n",
      "         -1.1475, -0.7889,  1.0409, -0.9129,  1.4328,  1.3812,  2.3747,\n",
      "          4.4442, -0.0888,  1.2008,  0.0036, -0.1236, -0.7256,  2.3400,\n",
      "         -0.4980,  1.1811,  0.8892, -2.2510,  3.2610,  0.1681,  0.4715,\n",
      "         -0.7029, -0.2430, -0.1518,  0.3784,  0.6200, -0.5552, -1.2072,\n",
      "         -1.1272, -0.3386, -0.2625,  1.4677, -0.0775,  2.3136,  0.3906,\n",
      "         -0.2415, -1.4832,  1.2398, -1.3270,  1.0273,  0.4377,  2.3668,\n",
      "         -0.7057,  0.6574,  0.5049, -1.0716,  1.7783,  0.5027,  1.4007,\n",
      "         -0.4994,  4.7294, -0.2264, -2.2616, -1.4082, -0.8999,  0.3088,\n",
      "          3.5465, -0.1945, -0.5648,  0.4963,  0.0195,  0.0151,  2.1831,\n",
      "         -0.5830, -2.8263, -0.9655,  4.9842, -2.8261, -0.6567,  2.3589,\n",
      "         -0.0860, -1.2747, -0.6183,  2.4933, -0.0895,  0.2494,  1.7347,\n",
      "          0.9046, -0.8414,  1.0291,  2.0750,  1.0215,  1.1930,  0.8074,\n",
      "         -0.1330,  0.6473, -1.5629, -0.7666, -0.8425,  1.6823, -0.5107,\n",
      "          1.8324, -1.5384, -1.2937,  0.0300,  0.4505,  0.1919, -0.3228,\n",
      "          1.6431, -1.5711,  0.0290,  0.3324,  0.3442, -1.4618,  2.2472,\n",
      "          1.8381,  1.1276, -1.3319,  3.7061,  0.6979,  0.5534, -0.3741,\n",
      "         -0.8856,  1.3238,  1.6145,  0.8072,  2.8743,  1.6439,  2.3642,\n",
      "          0.0026, -1.3368, -0.1801,  1.4212,  1.3259, -0.6911,  4.2735,\n",
      "         -1.1705,  1.5678,  3.4050, -0.6119,  0.6243,  1.7782,  1.3235,\n",
      "          1.3213, -0.8284,  0.7209,  1.1026,  2.0879,  0.7983,  1.3452,\n",
      "         -0.1604, -1.3269, -2.5211,  1.5583, -1.1629, -0.8835,  1.0130,\n",
      "          2.7776,  0.7498,  1.5470,  0.6924, -1.2064,  1.9050,  2.1715,\n",
      "          0.0491,  0.3212,  0.4456,  1.9206,  0.4366, -0.6805,  1.5774,\n",
      "          0.3925,  0.5758,  0.5819,  1.8518, -0.8815, -0.8015,  1.4852,\n",
      "          1.4662,  0.9548, -0.1154, -1.5562,  2.3819, -1.2740, -1.0478,\n",
      "          3.4820, -0.5447, -1.5655, -1.7010,  2.4828, -0.9087,  1.9686,\n",
      "         -0.0578, -0.4216,  1.5760, -0.0769,  3.0589, -0.4007, -0.7003,\n",
      "         -0.5016,  0.0342,  0.7663, -2.1343, -0.6526,  0.3773, -0.1383,\n",
      "          0.8298,  0.8768,  0.5738, -1.6092, -0.7014, -0.3821, -0.0211,\n",
      "          0.8370, -0.7406, -0.6581,  3.5822, -0.0047, -0.6730,  0.3241,\n",
      "         -0.7120, -2.3001,  0.4540,  0.3971,  2.7596, -0.5146, -1.7291,\n",
      "          0.6985, -0.7166, -0.3149,  0.9548,  0.5958, -2.1404,  0.1252,\n",
      "          0.2661,  0.9672,  1.9930,  0.4777,  1.1050,  1.7117, -0.5034,\n",
      "         -0.6667, -2.7118,  0.1396, -2.1081, -0.3382,  1.2087, -0.8079,\n",
      "          2.8674,  0.8494,  1.4275, -3.0910,  0.5149, -1.5442, -0.7759,\n",
      "          0.1431, -2.0204, -1.4468,  1.0482, -0.1973,  1.5969, -1.6597,\n",
      "          4.5604,  0.6913,  1.0388,  1.2293, -1.5886, -0.0038,  5.5464,\n",
      "          0.5897, -1.4163, -0.7785, -0.2823,  0.0749,  1.7101,  2.5479,\n",
      "          0.9326,  2.7131, -0.9725,  1.6823,  0.7889,  1.5443, -0.8682,\n",
      "          1.3473, -1.5404,  0.5448,  1.0113,  0.9210,  1.6085, -1.4865,\n",
      "          0.6008,  0.8749, -0.5371, -0.4988,  0.1766,  0.3575,  1.3646,\n",
      "          0.1678,  0.7918,  2.5920,  0.6354,  0.4284,  2.7614, -0.2061,\n",
      "          3.2809,  0.4694, -0.4375, -2.4419,  0.6229,  2.5569,  1.1953,\n",
      "          1.5805,  2.3144,  0.7765,  0.6723, -0.3221,  1.0167,  2.1020,\n",
      "          1.2455,  1.1894,  0.9854,  3.4301,  0.0998,  2.2911,  2.9340,\n",
      "          1.3234,  2.3657,  0.0564, -0.8110, -1.6653, -0.1655,  1.3496,\n",
      "          0.8204,  2.8736,  1.0765, -0.2743, -0.6592, -0.1799,  2.9148,\n",
      "         -1.2814, -0.4491, -0.4727,  0.0136,  1.8907, -1.6134,  0.5576,\n",
      "         -0.1071, -1.0229,  0.0103, -0.9648,  0.9982,  3.7879, -2.4276,\n",
      "         -1.0036,  0.1266,  1.8210, -1.8346,  0.6580,  1.8986, -2.4212,\n",
      "         -1.5779,  3.0181,  0.2907,  0.5146,  1.2154,  0.7786,  0.9479,\n",
      "          0.6312,  1.9738,  2.5234,  0.0153,  2.1692,  1.1847, -0.4163,\n",
      "         -1.9392,  0.8649,  0.7240, -0.5122,  1.3075,  0.4992,  0.9716,\n",
      "          1.5656,  0.1109, -2.7422,  0.2516,  1.6166,  0.6740, -1.4280,\n",
      "          0.7332, -0.6532, -0.8520, -1.6579, -1.6581, -0.5944, -0.6538,\n",
      "          2.5092,  0.4009,  1.4203, -0.8359,  1.1281, -2.0231, -2.8544,\n",
      "          0.1534,  0.4028,  0.2232, -1.5495,  1.4526,  0.7472, -1.4612,\n",
      "          1.3436,  1.3873,  1.1214,  2.3555,  0.2198,  0.5551, -0.7155,\n",
      "         -0.9551, -2.0277,  1.1820,  1.6009,  3.8343,  2.0339, -1.7935,\n",
      "          0.6322,  0.6745,  1.9212,  0.5497, -2.0991, -1.0324,  0.7513,\n",
      "          1.4171,  3.9786,  1.2472,  2.2838,  1.5733,  0.3036, -0.4905,\n",
      "         -0.3211,  4.6900,  0.3849, -0.3120, -1.8911, -0.9484, -0.8193,\n",
      "         -0.0306,  0.8050, -0.1678,  0.1329,  1.6419,  1.4067, -1.3214,\n",
      "         -0.6677, -0.0651, -1.9758, -1.6256,  0.3454,  0.8028, -1.5236,\n",
      "         -0.0482,  0.0186, -2.1097, -2.3013, -1.3864, -0.8520,  1.9893,\n",
      "         -1.6376,  2.4146,  0.0177, -0.0518, -1.1161, -1.1153, -3.4326,\n",
      "         -0.1651, -3.8985, -1.4782, -1.0024, -0.0754, -0.3138,  0.2686,\n",
      "          0.8030, -0.2752, -1.0592,  1.6195, -1.4559, -0.5987, -2.5144,\n",
      "         -2.7708, -0.7026, -2.2544,  0.3678,  0.0058, -1.6867, -3.2394,\n",
      "          1.4153, -0.6014, -0.3944, -0.0526, -2.9116,  2.4191, -2.0334,\n",
      "          0.5758, -2.1482, -0.3707, -2.2018, -1.1309, -0.5394, -1.0275,\n",
      "         -0.9966, -0.4936, -0.0245,  0.0567, -0.7253, -0.6778, -0.0574,\n",
      "         -0.1903,  0.0669,  0.0284,  0.2996,  0.1114, -1.6008, -2.5325,\n",
      "         -2.1807, -2.6157, -0.8192, -0.5650, -1.1319,  1.5756]])\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 3, 224, 224)\n",
    "print(model_alexnet(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.00000e+05 *\n",
      "       3.3377)\n",
      "conv1.bias.grad before backward\n",
      "None\n",
      "conv1.bias.grad after backward\n",
      "tensor([-17.9168,  -0.0625, -10.7579,   2.0432, -62.3260, -58.9705,\n",
      "        -20.9254,  -8.7410,  24.5047,  22.4319,   6.4826,  11.8428,\n",
      "          0.3501,  -0.5553,  22.4283,   5.5764, -28.9538,  34.4560,\n",
      "        -11.0774,   3.8307,  -4.5126, -17.0148,  20.0543,  -0.5957,\n",
      "         -4.9456,   0.7406, -16.4618,  13.4512,  -1.9954,  17.6860,\n",
      "         11.6637,  -0.8091, -10.9502,  13.3491,  -8.3213,   0.9344,\n",
      "          1.4198,  15.4449, -22.1429, -15.8686,  13.8913,   1.5511,\n",
      "          5.0415,  41.4938, -27.3529, -19.8215,  -0.8137, -20.8404,\n",
      "         -3.9847,  -3.5817, -18.0827,   0.0204, -10.6890,  24.5702,\n",
      "          1.2333, -40.8597, -54.4634,   6.1019,  22.3676,  31.8456,\n",
      "        -25.1448,  26.5563,  51.6522, -25.9091])\n"
     ]
    }
   ],
   "source": [
    "output = model_alexnet(input)\n",
    "target = torch.arange(1, 1001)  # a dummy target, for example\n",
    "target = target.view(1, -1)  # make it the same shape as output\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)\n",
    "\n",
    "model_alexnet.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(model_alexnet.features[0].bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(model_alexnet.features[0].bias.grad)\n",
    "\n",
    "learning_rate = 0.01\n",
    "for f in model_alexnet.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(model_alexnet.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = model_alexnet(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning\n",
    "Volviendo a la resNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ants', 'bees']\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = '/media/julio/DATA/Ayudantia/DeepLearning/Tarea2/hymenoptera_data'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "print(class_names)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 0.5872 Acc: 0.7090\n",
      "val Loss: 0.3524 Acc: 0.8693\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 0.6807 Acc: 0.7459\n",
      "val Loss: 0.3512 Acc: 0.8824\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 0.5555 Acc: 0.7541\n",
      "val Loss: 0.7107 Acc: 0.7451\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 0.4910 Acc: 0.7787\n",
      "val Loss: 0.2664 Acc: 0.8889\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 0.5400 Acc: 0.7664\n",
      "val Loss: 0.3216 Acc: 0.8758\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 0.5663 Acc: 0.7910\n",
      "val Loss: 0.3162 Acc: 0.8824\n",
      "\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 0.5255 Acc: 0.7582\n",
      "val Loss: 0.2417 Acc: 0.9150\n",
      "\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 0.2440 Acc: 0.8975\n",
      "val Loss: 0.1852 Acc: 0.9346\n",
      "\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 0.2969 Acc: 0.8607\n",
      "val Loss: 0.2276 Acc: 0.8954\n",
      "\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 0.3404 Acc: 0.8484\n",
      "val Loss: 0.2049 Acc: 0.9216\n",
      "\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 0.2720 Acc: 0.9016\n",
      "val Loss: 0.1950 Acc: 0.9346\n",
      "\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 0.2215 Acc: 0.9016\n",
      "val Loss: 0.1884 Acc: 0.9281\n",
      "\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 0.2236 Acc: 0.9057\n",
      "val Loss: 0.2168 Acc: 0.9150\n",
      "\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 0.3381 Acc: 0.8443\n",
      "val Loss: 0.1913 Acc: 0.9281\n",
      "\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 0.3081 Acc: 0.8648\n",
      "val Loss: 0.1908 Acc: 0.9281\n",
      "\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 0.3079 Acc: 0.8689\n",
      "val Loss: 0.1867 Acc: 0.9346\n",
      "\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 0.3681 Acc: 0.8033\n",
      "val Loss: 0.1918 Acc: 0.9346\n",
      "\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 0.2294 Acc: 0.9221\n",
      "val Loss: 0.2196 Acc: 0.9150\n",
      "\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 0.2188 Acc: 0.9139\n",
      "val Loss: 0.1987 Acc: 0.9281\n",
      "\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 0.2470 Acc: 0.8893\n",
      "val Loss: 0.1935 Acc: 0.9281\n",
      "\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 0.2879 Acc: 0.8648\n",
      "val Loss: 0.2137 Acc: 0.9150\n",
      "\n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 0.2563 Acc: 0.8975\n",
      "val Loss: 0.1840 Acc: 0.9281\n",
      "\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 0.2893 Acc: 0.8607\n",
      "val Loss: 0.1984 Acc: 0.9346\n",
      "\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 0.2476 Acc: 0.8811\n",
      "val Loss: 0.1833 Acc: 0.9346\n",
      "\n",
      "Epoch 24/24\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-195:\n",
      "Process Process-193:\n",
      "Process Process-196:\n",
      "Process Process-194:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/media/julio/DATA/Ayudantia/DeepLearning/Tarea2/env/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/media/julio/DATA/Ayudantia/DeepLearning/Tarea2/env/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/media/julio/DATA/Ayudantia/DeepLearning/Tarea2/env/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/media/julio/DATA/Ayudantia/DeepLearning/Tarea2/env/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-cc88ea5f8bd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n\u001b[0;32m----> 2\u001b[0;31m                        num_epochs=25)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-63-77b76ce2b63b>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     37\u001b[0m                     \u001b[0;31m# backward + optimize only if in training phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m                         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/julio/DATA/Ayudantia/DeepLearning/Tarea2/env/lib/python3.5/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/julio/DATA/Ayudantia/DeepLearning/Tarea2/env/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = torchvision.models.resnet18(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as\n",
    "# opoosed to before.\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n",
    "\n",
    "model_conv = train_model(model_conv, criterion, optimizer_conv,\n",
    "                         exp_lr_scheduler, num_epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplos para la segunda Tarea\n",
    "\n",
    "La tarea consiste en identificar el pais de origen de nombres, considerando que existe 18 paises diferentes.\n",
    "\n",
    "Esto lo vamos a hacer a nivel de caracteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/names/Arabic.txt', 'data/names/Chinese.txt', 'data/names/Czech.txt', 'data/names/Dutch.txt', 'data/names/English.txt', 'data/names/French.txt', 'data/names/German.txt', 'data/names/Greek.txt', 'data/names/Irish.txt', 'data/names/Italian.txt', 'data/names/Japanese.txt', 'data/names/Korean.txt', 'data/names/Polish.txt', 'data/names/Portuguese.txt', 'data/names/Russian.txt', 'data/names/Scottish.txt', 'data/names/Spanish.txt', 'data/names/Vietnamese.txt']\n",
      "Slusarski\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "\n",
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "print(findFiles('data/names/*.txt'))\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "print(unicodeToAscii('Ślusàrski'))\n",
    "\n",
    "# Build the category_lines dictionary, a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "for filename in findFiles('data/names/*.txt'):\n",
    "    category = filename.split('/')[-1].split('.')[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "n_categories = len(all_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora transformamos los nombres en tensores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])\n",
      "torch.Size([5, 1, 57])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "print(letterToTensor('J'))\n",
    "\n",
    "print(lineToTensor('Jones').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "n_hidden = 128\n",
    "rnn = RNN(n_letters, n_hidden, n_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.8960, -2.9029, -2.8845, -2.8476, -2.9815, -2.7949, -2.8907,\n",
      "         -2.8361, -2.8249, -2.8032, -2.9560, -2.9359, -2.8181, -2.9227,\n",
      "         -2.9040, -2.9587, -2.9078, -2.9924]])\n"
     ]
    }
   ],
   "source": [
    "input = letterToTensor('A')\n",
    "hidden =torch.zeros(1, n_hidden)\n",
    "\n",
    "output, next_hidden = rnn(input, hidden)\n",
    "\n",
    "input = lineToTensor('Albert')\n",
    "hidden = torch.zeros(1, n_hidden)\n",
    "\n",
    "output, next_hidden = rnn(input[0], hidden)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('French', 5)\n"
     ]
    }
   ],
   "source": [
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return all_categories[category_i], category_i\n",
    "\n",
    "print(categoryFromOutput(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category = Greek / line = Stavropoulos\n",
      "category = Irish / line = William\n",
      "category = French / line = Levesque\n",
      "category = Dutch / line = Otten\n",
      "category = French / line = Samson\n",
      "category = German / line = Vonnegut\n",
      "category = Scottish / line = Williamson\n",
      "category = French / line = Forest\n",
      "category = Korean / line = Noh\n",
      "category = Russian / line = Shirinyants\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "def randomTrainingExample():\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "    line_tensor = lineToTensor(line)\n",
    "    return category, line, category_tensor, line_tensor\n",
    "\n",
    "for i in range(10):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    print('category =', category, '/ line =', line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
    "\n",
    "def train(category_tensor, line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "\n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(-learning_rate, p.grad.data)\n",
    "\n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 5% (0m 4s) 1.9574 Long / Korean ✗ (Chinese)\n",
      "10000 10% (0m 8s) 3.0889 Cruz / Korean ✗ (Portuguese)\n",
      "15000 15% (0m 12s) 1.3388 Aswad / Arabic ✓\n",
      "20000 20% (0m 16s) 0.7766 Voltolini / Italian ✓\n",
      "25000 25% (0m 20s) 2.9906 Poulin / Irish ✗ (French)\n",
      "30000 30% (0m 24s) 1.6639 Anopriev / French ✗ (Russian)\n",
      "35000 35% (0m 29s) 1.5852 Cucinotta / Spanish ✗ (Italian)\n",
      "40000 40% (0m 33s) 2.5500 Kazmier / German ✗ (Czech)\n",
      "45000 45% (0m 37s) 1.1467 Offermans / Dutch ✓\n",
      "50000 50% (0m 41s) 0.3192 Karahalios / Greek ✓\n",
      "55000 55% (0m 45s) 2.0760 Neumann / French ✗ (German)\n",
      "60000 60% (0m 49s) 3.0403 Openshaw / Irish ✗ (English)\n",
      "65000 65% (0m 53s) 5.3111 Ruzzier / Czech ✗ (Italian)\n",
      "70000 70% (0m 57s) 1.4699 Geroux / Arabic ✗ (French)\n",
      "75000 75% (1m 1s) 2.2716 Bosch / Scottish ✗ (German)\n",
      "80000 80% (1m 4s) 3.2323 Smit / German ✗ (Dutch)\n",
      "85000 85% (1m 8s) 0.3407 D'cruze / Portuguese ✓\n",
      "90000 90% (1m 12s) 0.7144 Srour / Arabic ✓\n",
      "95000 95% (1m 15s) 0.0213 Kawasaki / Japanese ✓\n",
      "100000 100% (1m 19s) 0.6712 Stewart / Scottish ✓\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "n_iters = 100000\n",
    "print_every = 5000\n",
    "plot_every = 1000\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output, loss = train(category_tensor, line_tensor)\n",
    "    current_loss += loss\n",
    "\n",
    "    # Print iter number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> Dovesky\n",
      "(-0.75) Czech\n",
      "(-0.82) Russian\n",
      "(-2.89) English\n",
      "\n",
      "> Jackson\n",
      "(-0.27) Scottish\n",
      "(-2.00) English\n",
      "(-3.47) Czech\n",
      "\n",
      "> Alvaro\n",
      "(-0.66) Italian\n",
      "(-1.14) Spanish\n",
      "(-2.37) Portuguese\n"
     ]
    }
   ],
   "source": [
    "# Just return an output given a line\n",
    "def evaluate(line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "\n",
    "    return output\n",
    "\n",
    "def predict(input_line, n_predictions=3):\n",
    "    print('\\n> %s' % input_line)\n",
    "    with torch.no_grad():\n",
    "        output = evaluate(lineToTensor(input_line))\n",
    "\n",
    "        # Get top N categories\n",
    "        topv, topi = output.topk(n_predictions, 1, True)\n",
    "        predictions = []\n",
    "\n",
    "        for i in range(n_predictions):\n",
    "            value = topv[0][i].item()\n",
    "            category_index = topi[0][i].item()\n",
    "            print('(%.2f) %s' % (value, all_categories[category_index]))\n",
    "            predictions.append([value, all_categories[category_index]])\n",
    "\n",
    "predict('Dovesky')\n",
    "predict('Jackson')\n",
    "predict('Alvaro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
