{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"hWBgnPySifAX"},"outputs":[],"source":["import pandas as pd\n","import os\n","import re\n","import numpy as np\n","from numpy import log \n","\n","#librerias para texto\n","import nltk #Natural Language Tool Kit\n","\n","#importamos tokenizador de texto\n","from nltk.tokenize import word_tokenize \n","\n","#stemmer mediate algoritmo de porter\n","from nltk.stem import PorterStemmer \n","\n","#stemmer de libreria snowball\n","from nltk import SnowballStemmer \n","\n","#para conteo de vectores de texto\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","#para reemplazo de caracteres especiales latinos\n","import unidecode \n","import unicodedata\n","\n","#libreria gensim para complementar trabajo con texto\n","import gensim\n","\n","#modulo LDA de sklearn\n","from sklearn.decomposition import LatentDirichletAllocation\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jq2ZV6EjifAf"},"outputs":[],"source":["#Cambiamos directorio\n","dir=os.chdir('C:/Users/rfern/Desktop/Modulo 9/Codigos y datos')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tMyibr72ifAg"},"outputs":[],"source":["#importamos modulo para lectura del pdf\n","from PyPDF2 import PdfFileReader\n","file = open('new-testament-83291-spa.pdf', 'rb')\n","#cargamos pdf en un objeto PyPDF2.pdf.PdfFileReader\n","reader = PdfFileReader(file)\n","print(reader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b8oH_ej3ifAh"},"outputs":[],"source":["#creamos string vacío para almacenar resultados\n","resultado = '' \n","\n","#generamos ciclo para cada numero de pagina\n","for i in range(reader.numPages ):\n","    \n","    #creamos escalar que indexa el numero de pagina\n","    pagina = reader.getPage(i)\n","    \n","    #sumamos el texto extraido para la pagina indexada al resultado\n","    resultado += ' '+ pagina.extractText() + ' '"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1QfGAlB0ifAj"},"outputs":[],"source":["regex=r'(?<=\\d\\s).*?(?=\\.)'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ktaAHu-8ifAk"},"outputs":[],"source":["#implementamos la busqueda mediante metodo .findall\n","versiculos=re.findall(regex, resultado, re.DOTALL)\n","\n","#imprimimos el numero de articulos de la busqueda\n","print(len(versiculos), '\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"ejH5iITeifAl"},"outputs":[],"source":["#imprimimos cada uno de los articulos de la busqueda\n","for vers in range(len(versiculos)):\n","    print(vers+1, versiculos[vers], '\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"225eENkbifAn"},"outputs":[],"source":["#Pasamos la lista de articulos a un dataframe\n","df=pd.DataFrame(versiculos, columns=['contenido_crudo'])\n","df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"piRvTthVifAp"},"outputs":[],"source":["df.head(50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GwP8jrkbifAq"},"outputs":[],"source":["#importamos stopwords\n","from nltk.corpus import stopwords \n","\n","#descargamos stopwords\n","nltk.download('stopwords')\n","\n","#creamos lista de stopwords en castellano\n","stopwords=stopwords.words('spanish')\n","\n","#normalizamos stopwords, removemos tildes y otros caracteres latinos para el match con texto normalizado\n","#creamos lista vacía que almacenara palabras normalizadas\n","stop_words=[]\n","#ciclo en el que cada palabra\n","for word in stopwords:\n","    #es normalizada\n","    word_norm = unicodedata.normalize('NFD', word).encode('ascii', 'ignore').decode(\"utf-8\")\n","    \n","    #y almacenada en la nueva lista\n","    stop_words.append(word_norm)\n","\n","#Con la lematización pasa algo similar, cortar palabras puede hacernos perder su riqueza.\n","snowball = SnowballStemmer(language='spanish')\n","#instanciamos el lematizador de porter\n","#porter= PorterStemmer(languaje='spanish')\n","\n","#creamos funcion que limpia y normaliza texto de la descripcion web\n","def limpieza(texto):\n","    #pasamos a minusculas\n","    texto_norm=texto.lower()\n","    \n","    #removemos espacios al final y al inicio de cada cadena\n","    texto_norm=texto_norm.strip()\n","    \n","    #removemos dobles espacios\n","    texto_norm=re.sub('\\s',' ', texto_norm)\n","    \n","    #nuevo_texto = unidecode.unidecode(nuevo_texto)\n","    texto_norm = unicodedata.normalize('NFD', texto_norm).encode('ascii', 'ignore').decode(\"utf-8\")\n","    \n","    #retenemos caracteres alfanumericos: ojo que incluimos la ñ\n","    texto_norm=re.sub('[^A-Za-z]+', ' ', texto_norm)\n","    \n","    #tokenizamos texto: convertir en una lista una cadena\n","    texto_norm = word_tokenize(texto_norm)\n","    \n","   \n","    #removemos stopwords\n","    texto_norm=[word for word in texto_norm if not word in stop_words]\n","    \n","    #lematizamos texto con el stemmer de snowbal\n","    #texto_norm=[snowball.stem(word) for word in texto_norm]\n","    \n","    # Eliminación de terminos de largo 1\n","    texto_norm = [token for token in texto_norm if len(token) > 3] \n","   \n","    #resultado de la funcion\n","    return(texto_norm)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SvBYBQwNifAs"},"outputs":[],"source":["#implementamos la funcion de limpieza y normalizacion\n","df['contenido_limpio'] = df['contenido_crudo'].apply(lambda x: limpieza(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sY4zcUl6ifAt"},"outputs":[],"source":["from nltk.tokenize.treebank import TreebankWordDetokenizer\n","detok=TreebankWordDetokenizer()\n","df['contenido_limpio_detok']=df['contenido_limpio'].map(lambda x: detok.detokenize(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"UWZ3_fYrifAu"},"outputs":[],"source":["df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i9uuMtlhifAv"},"outputs":[],"source":["df['largo'] = df['contenido_limpio'].map(lambda x: len(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"GLrecbrMifAv"},"outputs":[],"source":["df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vVNoIARQifAw"},"outputs":[],"source":["df.largo.describe(percentiles=[.1,.2,.3,.5,.6,.7,.8,.9])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tEvBT5-cifAw"},"outputs":[],"source":["df=df[df['largo']>=4]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BSOtfybaifAx"},"outputs":[],"source":["#otra manera de preprocesar el texto\n","from gensim.utils import simple_preprocess\n","def sent_to_words(sentences):\n","    for sentence in sentences:\n","        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  \n","        \n","data = df.contenido_limpio_detok.values.tolist()\n","data_words = list(sent_to_words(data))\n","print(data_words[:1][0][:30])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jNz-KzqeifAx"},"outputs":[],"source":["#diseñamos funcion para la generacion de ngramas\n","def generate_N_grams(text,ngram):\n","    temp=zip(*[text[i:] for i in range(0,ngram)])\n","    ans=[' '.join(ngram) for ngram in temp]\n","    return ans\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iS-GB-lqifAy"},"outputs":[],"source":["#creamos lista de trigramas\n","data_ngram=[]\n","for i in data_words:\n","    t=generate_N_grams(i, 2)\n","    data_ngram.append(t)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"hR1TUiKZifAz"},"outputs":[],"source":["data_ngram"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"_owxxmfhifAz"},"outputs":[],"source":["#importamos modulo gensim para la generacion del corpus\n","import gensim.corpora as corpora\n","# creamos diccionario\n","id2word = corpora.Dictionary(data_ngram)\n","\n","# creamos corpus\n","texts = data_ngram\n","\n","# creamos DTM\n","corpus = [id2word.doc2bow(text) for text in texts]\n","\n","# Vista del corpus para el primer elemento\n","print(corpus[:1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ibyv-uMyifA0"},"outputs":[],"source":["#Contabilizamos el numero de nucleos de la CPU\n","import multiprocessing\n","workers=multiprocessing.cpu_count()\n","workers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Gj3UFQAifA1"},"outputs":[],"source":["#implementamos modelo de latent diriechlect allocation\n","lda_model = gensim.models.LdaMulticore(corpus=corpus,     #ingresamos corpus\n","                                       workers=workers-1 ,#numero de nucleos para procesamiento paralelo\n","                                       id2word=id2word,   #ingresamos la indexacion de ngramas\n","                                       num_topics=8,      #numero de topidos\n","                                       random_state=100,  #semilla aleatoria para replicabilidad de resultados\n","                                       chunksize=10,      #numero de documentos que serán utilizados en cada fase de entrenamient\n","                                       alpha=0.5,         #parametro alfa\n","                                       eta=0.3)           #    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tP4TnknPifA1"},"outputs":[],"source":["#calculamos el indice de coherencia\n","from gensim.models import CoherenceModel\n","coherencia_model_lda = CoherenceModel(model=lda_model, \n","                                      texts=data_ngram, \n","                                      dictionary=id2word, \n","                                      coherence='c_v')\n","\n","#obtenemos el indice de coherencia\n","coherencia = coherencia_model_lda.get_coherence()\n","\n","#imprimimos resultado\n","print(coherencia)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"wpNM0aflifA2"},"outputs":[],"source":["#importamos librerias y modulos para la visualizacion\n","import pyLDAvis\n","import pyLDAvis.gensim_models\n","\n","import pyLDAvis\n","import pyLDAvis.sklearn\n","pyLDAvis.enable_notebook()\n","# visualizamos topicos\n","vis= pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n","pyLDAvis.save_html(vis, 'modelo.html')"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"RZCugAm0ifA2"},"outputs":[],"source":["pyLDAvis.display(vis)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b8nFQiYBifA3"},"outputs":[],"source":["# Funcion para implementacion de modelo\n","\n","def compute_coherence_values(corpus, dictionary, k, a, b):\n","    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n","                                          workers=workers-1,\n","                                          id2word=dictionary,\n","                                           num_topics=k, \n","                                           random_state=100,\n","                                           chunksize=100,\n","                                           passes=10,\n","                                           alpha=a,\n","                                           eta=b)\n","    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_ngram, dictionary=id2word, coherence='c_v')\n","    \n","    return coherence_model_lda.get_coherence()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-qcjqNygifA3"},"outputs":[],"source":["import numpy as np\n","import tqdm\n","grid = {}\n","grid['Set de validacion'] = {}\n","# rango de topicos\n","min_topics = 2\n","max_topics = 10\n","step_size = 1\n","topics_range = range(min_topics, max_topics, step_size)\n","# rango de valores para parametro alfa\n","alpha = list(np.arange(0.1, 1, 0.1)) \n","alpha.append('symmetric')\n","alpha.append('asymmetric')\n","# rango de valores para parametro beta\n","beta = list(np.arange(0.2, 1, 0.1))\n","beta.append('symmetric')\n","## set de validacion\n","##numero de documentos\n","num_of_docs = len(corpus)\n","#\n","##utilizamos el 75% del corpus\n","corpus_sets = [gensim.utils.ClippedCorpus(corpus, int(num_of_docs*0.75)), corpus]\n","#\n","##titulos\n","corpus_title = ['75% Corpus', '100% Corpus']\n","#\n","##diccionario de resultados\n","model_results = {'Set de validacion': [],\n","                'Topicos': [],\n","                 'Alfa': [],\n","                 'Beta': [],\n","                 'Coherencia': []\n","                }\n","## iteraciones sobre los parametros\n","if 1 == 1:\n","    pbar = tqdm.tqdm(total=540)\n","    \n","    # iteramos sobre corpus de validacion\n","    for i in range(len(corpus_sets)):\n","        # iteramos sobre el numero de topicos\n","        for k in topics_range:\n","            # sobre parametro alfa\n","            for a in alpha:\n","                # sobre parametro beta\n","                for b in beta:\n","                    # calculamos el coeficiente de coherencia\n","                    cv = compute_coherence_values(corpus=corpus_sets[i], \n","                                                  dictionary=id2word, \n","                                                  k=k, \n","                                                  a=a, \n","                                                  b=b)\n","                    # guardamos resultados\n","                    model_results['Set de validacion'].append(corpus_title[i])\n","                    model_results['Topicos'].append(k)\n","                    model_results['Alfa'].append(a)\n","                    model_results['Beta'].append(b)\n","                    model_results['Coherencia'].append(cv)\n","                    \n","                    pbar.update(1)\n","    resultados=pd.DataFrame(model_results)\n","    resultados.to_csv('resultados_lda.csv', index=False)\n","    pbar.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AftcooxZifA4"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}